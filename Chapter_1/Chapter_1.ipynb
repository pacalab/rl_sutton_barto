{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 1 - Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1.1\n",
    "\n",
    "Self-Play: Suppose, instead of playing a fixed opponent, the RL algorithm described above played against itself. What to you think would happen in this case? Would it learn a different way of playing?\n",
    "\n",
    "Self-play is a well known technique for learning to play a game, especially when play with a real opponent (human or simulated) is scarce or difficult to obtain. In the particular case of tic-tac-toe described in the book, there would be a few design considerations first:\n",
    " - since the agent will play both X and Os, one on each side, there will be the need to represent the states accordingly. Half of the states will not be reachable to either side on any given game.\n",
    " - depending on the exact learning algorithm used, it might overfit to its own play style, i.e. it will find a policy that maximizes returns against it's own style, but not against other opponents, therefore exploration is a crucial aspect in this case.\n",
    " - if the learning algorithm relies on modelling the probabilities of the opponent making certain moves more often than others in a given state, the policy might settle into a sub-optimal policy.\n",
    "\n",
    " In conclusion, self-play is a powerful technique which can theoretically provide infinite experiences to learn from, but one needs to make some careful design choices to ensure the learned policy generalizes beyond self-play. In particular one needs to ensure that it doesn't settle into a sub-optimal equilibrium due to never visiting certain states."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1.2\n",
    "\n",
    "Symmetries: Many tic-tac-toe positions appear different but are really the same because of symmetries. How might we amend the RL algorithm described above to take advantage of this? In what ways would this improve it? Now thing again. Suppose the oponent did not take advantage of symmetries. In that case, should we? Is it true, then, that symmetrically equivalent positions should necessarily have the same value?\n",
    "\n",
    "Most practical situations have large state spaces (even infinite) and choosing those representations that benefit from natural symmetries in that environment means that we can reduce the size of the state space and speed up performance, by transferring any learning about a state to all other states that are equivalent to it via those symmetries. In the case of tic-tac-toe, the symmetries are obviously all 90-degrees rotations and mirroring about a central vertical or horizontal line, the diagonals as well as relative to the centre square. So choosing a representation that takes advantage of all these symmetries would definitely reduce the state (and action) space and improve learning from a computational perspective.\n",
    "\n",
    "However, relative to the further questions, if the opponent behaves differently on states that are equivalent in our representation, but not in theirs, this might be a detriment to our learning, as we won't take full advantage of that knowledge and potential inefficiencies in their play, while they can take advantage of our policy of playing consistently in all equivalent states."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Â Exercise 1.3\n",
    "\n",
    "Greedy Play: Suppose the RL player was _greedy_, that is, it always played the move that brought it to the position that it rated the best. Would it learn to play better or worse than a non-greedy player? What problems might occur?\n",
    "\n",
    "By using a greedy policy, especially at the beginning of learning, would, firstly, heavily depend on whatever initialisations were used for the position values, which could be wrong. Secondly, by always choosing greedily the action or position it might think best, it would prevent it from learning the value of other states or actions (which might be actually better). In general, the greedy strategy would likely settle to a sub-optimal policy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1.4\n",
    "\n",
    "Learning from Exploration: Suppose learning updates occurred after _all_ moves, including exploratory moves. If the step-size parameter is appropriately reduced over time, then the state values would converge to a set of probabilities. What are the two sets of probabilities computed when we do, and when we do not, learn from exploratory moves? Assuming that we do continue to make exploratory moves, which set of probabilities might be better to learn? Which would result in more wins?\n",
    "\n",
    "When we do backups from all moves, including exploratory ones, e.g. from $e$ to $c^*$ in Figure 1.1, the learned value of a state should tend (in the mean) towards the expected value over all possible moves from a given state. Additionaly if the step-size parameter $\\alpha$ is reduced towards zero, the learned values would converge with probability $1$ towards the expected value of taking all possible moves from that state. From that perspective, it would be different from the probability of winning if only the best move was used for updates. Formally, if we define the two sets of values $V_{best}$ and $V_{expl}$, their update rules become:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "V_{best}(s) & \\leftarrow V_{best}(s) + \\alpha \\left[ \\underset{s'} {\\max} V_{best}(s') - V_{best}(s) \\right] \\\\\n",
    "\\\\\n",
    "V_{expl}(s) & \\leftarrow V_{expl}(s) + \\alpha \\left[ \\mathbb{E}_{s'} V_{expl}(s') - V_{expl}(s) \\right]\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "And since by definition $\\mathbb{E}_{s} V(s) \\le \\underset{s} {\\max} V(s)$, it results that $V_{best} \\le V_{expl}$ for all $s$.\n",
    "\n",
    "However, if the plan is to keep making exploratory moves (for various reasons this might be a good decision, e.g. for non-stationary environments), then $V_{best}$ would have no relevance in practice, since what we'll observe will be $V_{expl}$, so it would make more sense to learn the latter and use it to get more wins overall."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1.5\n",
    "\n",
    "Other improvements: Can you think of other ways to improve the RL player? Can you think of any better way to solve the tic-tac-toe problem as posed?\n",
    "\n",
    "The RL player could be improved (or at least the learning sped up), if the intialization values would be closer to the theoretical probability of winning from every position, instead of the blanket value of $0.5$.\n",
    "\n",
    "Another improvement could be to additionally learn a model of the opponent's policy, i.e. learn the probabilities of the opponent making a certain move in a certain state.\n",
    "\n",
    "Given the relatively small state and action space, the game of tic-tac-toe could be also solved by theoretically playing every possible position. That is, programatically create a tree-like structure which can easily identify which are the best moves in every given state. This is the equivalent of finding the Nash equilibrium of the game, but notice that it might not be the optimum policy for us when the opponent is not making the optimal moves themselves, since there could be missed opportunities for our agent."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
