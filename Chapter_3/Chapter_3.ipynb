{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. The Reinforcement Learning Problem\n",
    "\n",
    "This notebook implements the exercises in Chapter 3 (1st edition)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3.1\n",
    "Other examples of RL tasks:\n",
    "- **Self driving cars:** Given the current position, speed, traffic conditions etc., determine the best sequence of actions to manoeuvre the car safely to the destination. Positive rewards for smooth riding, reaching destination and negative rewards for brusque manoeuvres (e.g. from sudden change in acceleration), accidents, illegal moves.\n",
    "\n",
    "- **A tutoring software:** Given access to a database of educational items, the tutor would choose the next best item to present to the pupil. The reward signals are the grades at exams.\n",
    "\n",
    "- **A logistic planner:** Given a complex list of deliveries, inventory, people, resources, cars, etc., i.e. the state, choose the best actions such as to maximize rewards -(time to delivery), -(resources used), -(user ratings)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3.2\n",
    "??? Perhaps an exception are goal-directed problems in which the assumptions of the MDP framework are not satisfied. For instance when the (stochastic) \"causality\" doesn't manifest in the relationship $\\{state,\\: action\\} \\to \\{reward,\\: next\\_state\\}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3.3\n",
    "The line between the agent and the environment should be drawn by taking into consideration multiple factors, like the available representations of states and actions. As mentioned at the end of Section 3.1, representation can strongly affect performance. Another consideration is to be given to the actual reward - for example in the case of driving - simply getting from A to B is not enough reward signal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3.4\n",
    "$$ R_t = r_{t+1} + \\gamma^{1} r_{t+2} + \\gamma^{2} r_{t+3} + ... + \\gamma^{T-t-1} r_{T} $$\n",
    "and since all $r_{t} = 0$, for all $t \\ne T$ and $r_{T} = -1$, then \n",
    "$$ R_t = - \\gamma^{T-t-1} $$\n",
    "which is maximised when $T$ is largest."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3.5\n",
    "The agent's implementation needs to be able to reach the exit of the maze such as to learn the (only) positive reward of that state. This could be a problem of balancing exploration with exploitation, since any finite maze can be eventually solved, provided enough exploration is carried out."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3.6\n",
    "Yes, the Markov state doesn't imply complete information about the state of your environment, like objects behind the camera in this case. As for not receiving images all day, one could consider that it is still access to the Markov state, as that's all you can know about the environment with the current senses (i.e. faulty camera)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3.7\n",
    "$$ \\mathcal{P}_{ss'}^{a} = \\sum_{r_i \\in \\mathcal{R}} Pr \\left\\{ s_{t+1} = s', r_{t+1} = r_i \\mid s_t = s, a_t = a \\right\\} $$\n",
    "$$ \\mathcal{R}_{ss'}^{a} = \\sum_{r_i \\in \\mathcal{R}} r_i \\cdot Pr \\left\\{ s_{t+1} = s', r_{t+1} = r_i \\mid s_t = s, a_t = a \\right\\} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3.8\n",
    "Write the Bellman equation for $ Q^{\\pi} (s, a) $:\n",
    "$$ Q^{\\pi} (s, a) = \\mathbb{E}_{\\pi} \\left\\{ \\sum_{k=0}^{\\infty} \\gamma^{k} r_{t+k+1} \\mid s_t = s, a_t = a \\right\\} $$\n",
    "$$ = \\mathbb{E}_{\\pi} \\left\\{ r_{t+1} + \\gamma \\sum_{k=0}^{\\infty} {\\gamma^k r_{t+k+2}} \\mid s_t = s, a_t = a \\right\\} \\text{, then sum over all next states } {s'} \\text{ from } {s} \\text{ after action } {a} $$\n",
    "$$ = \\sum_{s' \\in \\mathcal{S}} { \\mathcal{P}_{ss'}^{a} \\left\\{ \\mathbb{E} \\left[r_{t+1} \\mid s_t = s, a_t = a, s_{t+1} = s'\\right] + \\gamma \\mathbb{E}_{\\pi} \\left\\{ \\sum_{k=0}^{\\infty} {\\gamma^{k} r_{t+k+2}} \\mid s_t = s, a_t = a, s_{t+1} = s' \\right\\} \\right\\} } $$\n",
    "where the first term is $\\mathcal{R}_{ss'}^{a}$ and the second can be simplified given Markov property, i.e. $r_{t+k+2}$ depend only on $s_{t+1}$ for all $k \\geq 0$\n",
    "$$ = \\sum_{s' \\in \\mathcal{S}} {\\mathcal{P}_{ss'}^{a} \\left( \\mathcal{R}_{ss'}^{a} + \\gamma \\mathbb{E}_{\\pi} \\left\\{ \\sum_{k=0}^{\\infty} {\\gamma^{k} r_{t+k+2}} \\mid s_{t+1} = s' \\right\\} \\right)} \\text{, then sum over all } a' \\text{ from state } s' $$\n",
    "$$ = \\sum_{s' \\in \\mathcal{S}} {\\mathcal{P}_{ss'}^{a} \\left( \\mathcal{R}_{ss'}^{a} + \\gamma \\sum_{a' \\in \\mathcal{A}(s')} {\\pi(s', a') \\cdot \\mathbb{E}_{\\pi} \\left\\{ \\sum_{k=0}^{\\infty} \\gamma^{k} r_{t+k+2} \\mid s_{t+1} = s', a_{t+1} = a' \\right\\}} \\right) } $$\n",
    "$$ = \\sum_{s' \\in \\mathcal{S}} \\mathcal{P}_{ss'}^{a} \\left( \\mathcal{R}_{ss'}^{a} + \\gamma \\sum_{a' \\in \\mathcal{A(s')}} \\pi(s', a') \\cdot Q^{\\pi} (s', a') \\right) \\text{, since the inner expectation is by definition } Q^{\\pi} (s', a'). \\: \\square $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3.9\n",
    "Let's call the center state $center$ and the four neighbour states $up$, $down$, $left$, $right$, reachable after taking actions $north$, $south$, $east$ and $west$, respectively. Then from $\\text{(3.10)}$:\n",
    "$$ V_{\\pi} (center) = \\sum_{a \\in \\mathcal{A} (center)} \\pi(center, a) \\cdot \\sum_{s'} \\mathcal{P}_{center \\to s'}^{a} \\left[ \\mathcal{R}_{center \\to s'}^{a} + \\gamma V^{\\pi} (s') \\right] $$\n",
    "Since the policy $\\pi$ is random and there are $4$ actions in $\\mathcal{A}(center)$, we have $\\pi(center, a) = \\frac{1}{4}$ for all $a$. Also, the state transitions are deterministic given an action $a$, so $\\mathcal{P}_{center \\to s'}^{a} = 1$ if $a = north$ and $s' = up$, or $a = east$ and $s' = left$, etc. and $0$ otherwise. Also, all rewards of moving away from $center$ state are zero by definition of the problem, i.e. $\\mathcal{R}_{center \\to s'}^{a} = 0$ for all $s'$, so:\n",
    "$$ V_{\\pi} (center) = \\sum_{a \\in \\mathcal{A} (center)} \\frac{\\gamma}{4} \\sum_{s'} V^{\\pi} (s') $$\n",
    "As we mentioned above the states $s'$ are uniquely defined by the actions $a$, so:\n",
    "$$ V_{\\pi} (center) = \\frac{0.9}{4} \\left(2.3 + 0.4 + (-0.4) + 0.7\\right) \\approx \\boxed{0.7} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3.10\n",
    "Prove that the signs of individual rewards $r_t$ in a continuing task (e.g. gridworld) are not important, but only the intervals between them.\n",
    "<br>\n",
    "Starting from the definition of discounted returns in $(3.2)$:\n",
    "$$ R_t = \\sum_{k=0}^{\\infty} \\gamma^{k} r_{t+k+1} \\text{, by adding a constant } C \\text{ to all rewards we get:} $$\n",
    "\n",
    "$$ \n",
    "\\begin{aligned}\n",
    "    R_{t}^{new} &= \\sum_{k=0}^{\\infty} \\gamma^{k} (r_{t+k+1} + C) \\\\\n",
    "    &= \\sum_{k=0}^{\\infty} \\gamma^{k} r_{t+k+1} + C \\sum_{k=0}^{\\infty} \\gamma^{k} \\\\\n",
    "    &= R_t + C \\lim_{k \\to \\infty} \\frac {\\gamma^{k+1} - 1} {\\gamma - 1} \\\\\n",
    "    &= R_t + C \\frac {1} {1 - \\gamma} \\\\\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Thus to all state values a constant equal to $C \\frac {1} {1 - \\gamma}$ is added which doesn't affect the relative values between them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3.11\n",
    "Is it the same for an episodic task?\n",
    "<br>\n",
    "No, by using $(3.3)$ (corrected - see <a href=\"http://incompleteideas.net/book/first/errata.html\">official Errata</a>) for $T \\ne \\infty$:\n",
    "$$ R_t = \\sum_{k=0}^{T-t-1} \\gamma^{k} r_{t+k+1} \\text{, by adding C to each reward, we can write:} $$\n",
    "$$ \n",
    "\\begin{aligned}\n",
    "    R_t^{new} &= \\sum_{k=0}^{T-t-1} \\gamma^{k} \\left(r_{t+k+1} + C\\right) \\\\\n",
    "    &= \\sum_{k=0}^{T-t-1} \\gamma^{k} r_{t+k+1} + C \\sum_{k=0}^{T-t-1} \\gamma^{k} \\\\\n",
    "    &= \\begin{cases}\n",
    "        R_t + C \\frac {\\gamma^{T-t} - 1} {\\gamma - 1} \\qquad \\text{, if } \\gamma < 1 \\text{ or:} \\\\\n",
    "        R_t + C \\cdot (T - t) \\qquad \\text{, if } \\gamma = 1. \\\\\n",
    "    \\end{cases}\n",
    "\\end{aligned}\n",
    "$$\n",
    "In both cases, we notice that the return $R_t^{new}$ is now dependent on $t$ and the relative values between value states cannot remain constant."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3.12\n",
    "$$\n",
    "\\begin{aligned}\n",
    "    V^{\\pi} (s) &= \\mathbb{E}_{\\pi} \\left[ Q^{\\pi} (s_t, a_t) \\mid s_t = s \\right] \\\\\n",
    "    &= \\sum_{a \\in \\mathcal{A}(s)} \\pi(s, a) \\cdot Q^{\\pi} (s, a)\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3.13\n",
    "$$ \n",
    "\\begin{aligned}\n",
    "    Q^{\\pi} (s, a) &= \\mathbb{E} \\left[ r_{t+1} \\mid s_t = s, a_t = a \\right] + \\gamma \\cdot \\mathbb{E}_{\\pi} \\left[ V^{\\pi} (s_{t+1}) \\mid s_t = s, a_t = a \\right] \\\\\n",
    "    &= \\sum_{s' \\in \\mathcal{S}} \\mathcal{P}_{ss'}^{a} \\cdot \\mathcal{R}_{ss'}^{a} + \\gamma \\sum_{s' \\in \\mathcal{S}} \\mathcal{P}_{ss'}^{a} \\cdot V^{\\pi} (s') \\\\\n",
    "    &= \\sum_{s' \\in \\mathcal{S}} \\mathcal{P}_{ss'}^{a} \\left( \\mathcal{R}_{ss'}^{a} + \\gamma \\cdot V^{\\pi} (s') \\right) \\\\\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3.14\n",
    "We're asked to describe $V^*(s)$ for the golf example.\n",
    "Starting from the bottom image in Figure 3.6, which describes $Q^*(s, driver)$, the optimal value-state for any location inside zones marked with $-2$ and $-3$ is also $-2$ and $-3$, respectively, because the action under the optimal policy is to use the driver. For any location inside the green, the optimal policy is to use the putter, as depicted in the top image in Figure 3.6, for a value of $-1$ and of course the in-the-hole state has a value of $0$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3.15\n",
    "We're asked to describe the $Q^*(s, putter)$ for the golf example, i.e. the optimal action-value function if the first action is putting from any state $s$.\n",
    "<br>\n",
    "\n",
    "Let's work backwards from the green in the top image of Figure 3.6. It's easy to see that $Q^* (green, putter) = -1$. \n",
    "<br>\n",
    "\n",
    "Then going to any location marked as $-2$, using the putter first will get the ball onto the green, then we'll follow with $Q^* (green, putter) = -1$, so in total $Q^* (zone_{-2}, putter) = -2$.\n",
    "<br>\n",
    "\n",
    "Similarly, putting first from zone $-3$ will bring us to zone $-2$, from where we'll need another two strikes, either {drive, putter} or {putter, putter}, for a total of $Q^* (zone_{-3}, putter) = -3$.\n",
    "<br>\n",
    "\n",
    "For zone $-4$, using first a putter takes us to zone $-3$, but then we can use the driver to reach the green in one strike. So $Q^* (zone_{-4}, putter) = -3$.\n",
    "<br>\n",
    "\n",
    "For zone $-5$, using first a putter takes us to zone $-4$, which is inside the zone $-3$ in the bottom image, so using a driver next will take us to zone $-2$ (in the same bottom image), then to the hole in two strikes - {driver, putter}, so $Q^* (zone_{-5}, putter) = -4$.\n",
    "<br>\n",
    "\n",
    "For zone $-6$, using first a putter takes us to zone $-5$, which is similar to the situation above, so $Q^* (zone_{-6}, putter) = -4$.\n",
    "<br>\n",
    "\n",
    "A bit of special consideration for the sand: we know that by using a putter in the sand will leave us in the same spot, after which we can use the driver as per the optimal policy and get to the hole in two strikes, so in total $Q^*(sand, putter) = -1 + Q^*(sand, driver) = -3$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3.16\n",
    "Give the Bellman equation for $Q^*$ for the recycling robot.\n",
    "We need to write a system of equations similar to the one given in Example 3.11, but for $Q^*$ instead of $V^*$. We should end up with 2 states x 3 actions = 6 equations and 6 unknowns:\n",
    "\n",
    "$$ \\begin{aligned}\n",
    "\n",
    "&Q^* (h, s) = \\mathcal{P}_{hh}^{s} \\left[ \\mathcal{R}_{hh}^{s} + \\gamma \\max_{a' \\in \\{s, w, re\\}} Q^* (h, a') \\right] + \\mathcal{P}_{hl}^{s} \\left[ \\mathcal{R}_{hl}^{s} + \\gamma \\max_{a' \\in \\{s, w, re\\}} Q^* (l, a') \\right] \\\\\n",
    "\n",
    "&Q^* (h, w) = \\mathcal{P}_{hh}^{w} \\left[ \\mathcal{R}_{hh}^{w} + \\gamma \\max_{a' \\in \\{s, w, re\\}} Q^* (h, a') \\right] + \\mathcal{P}_{hl}^{w} \\left[ \\mathcal{R}_{hl}^{w} + \\gamma \\max_{a' \\in \\{s, w, re\\}} Q^* (l, a') \\right] \\\\\n",
    "\n",
    "&Q^* (h, re) = \\mathcal{P}_{hh}^{re} \\left[ \\mathcal{R}_{hh}^{re} + \\gamma \\max_{a' \\in \\{s, w, re\\}} Q^* (h, a') \\right] + \\mathcal{P}_{hl}^{re} \\left[ \\mathcal{R}_{hl}^{re} + \\gamma \\max_{a' \\in \\{s, w, re\\}} Q^* (l, a') \\right] \\\\\n",
    "\n",
    "&Q^* (l, s) = \\mathcal{P}_{lh}^{s} \\left[ \\mathcal{R}_{lh}^{s} + \\gamma \\max_{a' \\in \\{s, w, re\\}} Q^* (h, a') \\right] + \\mathcal{P}_{ll}^{s} \\left[ \\mathcal{R}_{ll}^{s} + \\gamma \\max_{a' \\in \\{s, w, re\\}} Q^* (l, a') \\right] \\\\\n",
    "\n",
    "&Q^* (l, w) = \\mathcal{P}_{lh}^{w} \\left[ \\mathcal{R}_{lh}^{w} + \\gamma \\max_{a' \\in \\{s, w, re\\}} Q^* (h, a') \\right] + \\mathcal{P}_{ll}^{w} \\left[ \\mathcal{R}_{ll}^{w} + \\gamma \\max_{a' \\in \\{s, w, re\\}} Q^* (l, a') \\right] \\\\\n",
    "\n",
    "&Q^* (l, re) = \\mathcal{P}_{lh}^{re} \\left[ \\mathcal{R}_{lh}^{re} + \\gamma \\max_{a' \\in \\{s, w, re\\}} Q^* (h, a') \\right] + \\mathcal{P}_{ll}^{re} \\left[ \\mathcal{R}_{ll}^{re} + \\gamma \\max_{a' \\in \\{s, w, re\\}} Q^* (l, a') \\right] \\\\\n",
    "\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Which can be solved by expanding the max's and using the values from Table 3.1 or the transition graph from Figure 3.3:\n",
    "\n",
    "$$ \\begin{aligned}\n",
    "\n",
    "&Q^* (h, s) = \\alpha \\left[ \\mathcal{R}^{s} + \\gamma \\max \\left. \\begin{cases} Q^* (h, s)\\\\ Q^* (h, w)\\\\ Q^* (h, re) \\end{cases} \\right\\} \\right] + (1 - \\alpha) \\left[ \\mathcal{R}^{s} + \\gamma \\max \\left. \\begin{cases} Q^* (l, s)\\\\ Q^* (l, w)\\\\ Q^* (l, re) \\end{cases} \\right\\} \\right] \\\\\n",
    "\n",
    "&Q^* (h, w) = \\mathcal{R}^{w} + \\gamma \\max \\left. \\begin{cases} Q^* (h, s)\\\\ Q^* (h, w)\\\\ Q^* (h, re) \\end{cases} \\right\\} \\\\\n",
    "\n",
    "&Q^* (h, re) = 0 \\\\\n",
    "\n",
    "&Q^* (l, s) = (1 - \\beta) \\left[ -3 + \\gamma \\max \\left. \\begin{cases} Q^* (h, s)\\\\ Q^* (h, w)\\\\ Q^* (h, re) \\end{cases} \\right\\} \\right] + \\beta \\left[ \\mathcal{R}^{s} + \\gamma \\max \\left. \\begin{cases} Q^* (l, s)\\\\ Q^* (l, w)\\\\ Q^* (l, re) \\end{cases} \\right\\} \\right] \\\\\n",
    "\n",
    "&Q^* (l, w) = \\mathcal{R}^{w} + \\gamma \\max \\left. \\begin{cases} Q^* (l, s)\\\\ Q^* (l, w)\\\\ Q^* (l, re) \\end{cases} \\right\\} \\\\\n",
    "\n",
    "&Q^* (l, re) = 0 + \\gamma \\max \\left. \\begin{cases} Q^* (h, s)\\\\ Q^* (h, w)\\\\ Q^* (h, re) \\end{cases} \\right\\} \\\\\n",
    "\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "and given any choice of $\\mathcal{R}^{s}$, $\\mathcal{R}^{w}$, $\\alpha$, $\\beta$ and $\\gamma$, we can uniquely solve the system. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3.17\n",
    "Compute the optimal value of the best state (position of A) in the gridworld by using the optimal policy and (3.2).\n",
    "\n",
    "We denote any state (i.e. position on the grid, other than $A$ and $A'$) as $G_{row, col} \\text { , where } row, col = \\overline{1,5}$. \n",
    "Since any action in state $A$ takes us deterministically to $A'$ and gives an immediate reward of $10$, and the optimal policy $\\pi^*$ is shown in Figure 3.8c, we can write:\n",
    "\n",
    "$$ \n",
    "\\begin{aligned}\n",
    "    V^*(A) &= 10 + \\gamma V^*(A') \\\\\n",
    "    &= 10 + \\gamma (0 + \\gamma V^*(G_{4, 2})) \\\\\n",
    "    &= 10 + \\gamma (0 + \\gamma (0 + \\gamma V^*(G_{3, 2}))) \\\\\n",
    "    &= 10 + \\gamma (0 + \\gamma (0 + \\gamma (0 + \\gamma V^*(G_{2, 2})))) \\\\\n",
    "    &= 10 + \\gamma (0 + \\gamma (0 + \\gamma (0 + \\gamma (0 + \\gamma V^*(A))))) \\\\\n",
    "    &= 10 + \\gamma^5 V^*(A) \\\\\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "From where we can solve for $V^*(A)$:\n",
    "\n",
    "$$ V^*(A) = \\frac {10} {1 - 0.9^5} \\approx \\boxed{24.419} \\qquad \\square $$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gi2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
