{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 5 - Monte Carlo Methods\n",
    "This chapter introduces MC methods, which differ from DP methods (Chapter 4) in a few key aspects:\n",
    "\n",
    "- It learns from experience (samples of sequences of states, actions and rewards from interacting with an environment or a simulation), not from complete knowledge of the environment, i.e. the values $\\mathcal{P}_{ss'}^a$ and $\\mathcal{R}_{ss'}^a$ are inaccessible.\n",
    "\n",
    "- It averages over complete returns from a given state to a terminal state, therefore the estimates for each state are independent, because it doesn't build upon the estimate of any other state, as is the case in DP. So MC doesn't bootstrap.\n",
    "\n",
    "- If a model of the environment is not available, it is more useful to estimate action values $Q^{\\pi}(s, a)$ instead of $V^{\\pi}(s)$. In the former case, the optimal next action in a given state $s$ is simply calculated as $\\underset{a} {\\arg \\max} Q(s, a)$, while knowing just $V(s)$ is not useful without being able to calculate the value of each action from that state."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 5.1\n",
    "Question on the approximate state-value functions for blackjack (Figure 5.2):\n",
    "\n",
    "Q: Why does the value function jump up for the last two rows in the rear? <br>\n",
    "A: Because when the player's sum (state) is 20 and 21, the policy prescribes _sticking_, with a high chance of a win being so close to 21. So the value of being in those states is high (if following this policy).\n",
    "\n",
    "Q: Why does it drop off for the last row on the left? <br>\n",
    "A: That corresponds to the dealer showing an _ace_, which puts it in a better position overall (as it can be used both as 1 or 11) regardless of the cards of the player. So the value of being in any of those states is correspondingly reduced for the player.\n",
    "\n",
    "Q: Why are the frontmost values higher in the upper diagram than in the lower? <br>\n",
    "A: Because all being equal, having a low sum (12, 13, ...) _with an usable ace_ is a better position for the player, since the policy prescribes _sticking_ in those states and the chances of overshooting 21 are lower due to the _ace_ potentially counted as 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 5.2\n",
    "What is the backup diagram for MC estimation of $Q^{\\pi}$? (analogous to Figure 5.3 for $V^{\\pi}$)\n",
    "\n",
    "The backup diagram for $Q^{\\pi}$ is similarly linear, since we only look at experience, i.e. pairs of $(state,\\: action)$ that were actually played / simulated. The only difference is that the nodes (except the terminal state) are of a single type, which is tuples of $(state,\\: action)$."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
