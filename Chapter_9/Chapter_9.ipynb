{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 9 - Planning and Learning\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 9.1\n",
    "\n",
    "The nonplanning method looks particularly poor in Figure 9.6 because it is a one-step method; a method using eligibility traces would do better. Do you think an eligibility trace method could do as well as the Dyna method? Explain why or why not.\n",
    "\n",
    "__Answer__: I expect eligibility traces methods to perform generally worse compared to Dyna-Q in this deterministic task (the Dyna Maze), but could be on par under certain conditions. In particular, there are three factors we should consider: the number of backup updates, the magnitude of the updates (controlled by $\\alpha$ and $\\lambda$) and the coverage of state-action space. Let's take them one by one.\n",
    "\n",
    "At each step taken in the real environment, Dyna-Q makes $N$ updates via planning, so in total $N T$ updates of the action values table $Q(\\cdot, \\cdot)$ per episode. Eligibility traces update all previous steps of the episode at each step, so roughly $1 + 2 + ... + T = \\dfrac {T(T-1)} {2}$ updates. So for long episodes, when $T > 2N$, eligibility traces seem to have the advantage of making more updates to $Q$ than Dyna.\n",
    "\n",
    "However, in terms of the magnitude of those updates, for this task where the only non-zero reward is received only by reaching the goal $G$, eligibility traces perform exponentially smaller updates as they move backwards in time, as controlled by the $\\lambda$ parameter. So the true values would be slow in propagating back to early states for a small $\\lambda$. Compare Figure 7.12 with Figure 9.6. But if $\\lambda \\approx 1$, the updates should be exactly the same as the planning updates from Dyna.\n",
    "\n",
    "Finally, we need to keep in mind that the updates in Dyna are randomly chosen from all previous transitions, which means that coverage of the state-action space could be broader early on in Dyna, compared to eligibility traces, which can only update previous steps from the current episode. However, Dyna can also waste large numbers of update steps later on, by randomly choosing them far from the optimal path, whereas eligibility traces would only update states that are closer to the optimal path (due to policy improvement over time).\n",
    "\n",
    "Given all these reasons, one would expect Dyna methods to perform better than eligibility traces, especially early on, due to faster convergence. But under certain conditions (long episodes and $\\lambda$ close to $1$) eligibility traces should be able to match its performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Â Exercise 9.2\n",
    "\n",
    "Why did the Dyna agent with exploration bonus, Dyna-Q+, perform better in the first phase as well as in the second phase of the blocking and shortcut experiment?\n",
    "\n",
    "__Answer__: An exploration bonus nudges the agent towards exploring more than relying on the randomness induced by the $\\epsilon$-greedy policy, thus discovering the optimal path faster (as well as accumulating more reward from the bonus itself). This is true both for the inital phase, when the optimal path is found, but even more so in the second phase, when the change in the environment needs a significant amount of exploration in order to find the new optimal paths (this is true for both the blocking and shortcut experiments)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 9.3\n",
    "\n",
    "Careful inspection of Figure 9.8 reveals that the difference between Dyna-Q and Dyna-Q+ narrowed slightly over the first part of the experiment. What is the reason for this?\n",
    "\n",
    "__Answer__: The advantage of exploration starts to wear out after the optimal path has been found, and exploitation is more rewarding. However in this experiment's setting, the extra bonus for exploration still maintains Dyna-Q+ above Dyna-Q."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 9.4\n",
    "\n",
    "The exploration bonus described above actually changes the estimated values of states and actions. Is this necessary? Supposed the bonus $\\kappa \\sqrt{n}$ was used not in backups, but solely in action selection. That is, suppose the action selected was always that for which $Q(s, a) + \\kappa \\sqrt{n_{sa}}$ was maximal. Carry out a gridworld experiment that tests and illustrates the strengths and weaknesses of this alternate approach.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class State:\n",
    "    def __init__(self, row, col):\n",
    "        self.row = row\n",
    "        self.col = col\n",
    "\n",
    "    def __eq__(self, other_state):\n",
    "        return isinstance(other_state, self.__class__) and self.row == other_state.row and self.col == other_state.col\n",
    "\n",
    "    def __hash__(self):\n",
    "        return hash((self.__class__, self.row, self.col))\n",
    "\n",
    "    def __str__(self):\n",
    "        return f'({self.row}, {self.col})'\n",
    "\n",
    "\n",
    "class Action:\n",
    "    def __init__(self, row, col):\n",
    "        self.row = row\n",
    "        self.col = col\n",
    "\n",
    "    def __eq__(self, other_action):\n",
    "        return isinstance(other_action, self.__class__) and self.row == other_action.row and self.col == other_action.col\n",
    "\n",
    "    def __hash__(self):\n",
    "        return hash((self.__class__, self.row, self.col))\n",
    "\n",
    "    def __str__(self):\n",
    "        return f'({self.row}, {self.col})'\n",
    "\n",
    "\n",
    "class GridWorld:\n",
    "    def __init__(self, rows, cols, inner_walls, start, goal):\n",
    "        self.rows = rows\n",
    "        self.cols = cols\n",
    "        self.walls = inner_walls  # a set of States\n",
    "        # add walls all around the grid too\n",
    "        for row in range(self.rows + 2):\n",
    "            self.walls.add(State(row, 0))\n",
    "            self.walls.add(State(row, self.cols + 1))\n",
    "        for col in range(self.cols + 2):\n",
    "            self.walls.add(State(0, col))\n",
    "            self.walls.add(State(self.rows + 1, col))\n",
    "        self.state = start\n",
    "        self.goal = goal\n",
    "\n",
    "    @property\n",
    "    def is_terminal(self):\n",
    "        return self.state == self.goal\n",
    "\n",
    "    def legal_actions(self, s: State = None):\n",
    "        return [\n",
    "            Action(1, 0),\n",
    "            Action(-1, 0),\n",
    "            Action(0, 1),\n",
    "            Action(0, -1),\n",
    "        ]\n",
    "\n",
    "    def step(self, a: Action):\n",
    "        next_state = State(self.state.row + a.row, self.state.col + a.col)\n",
    "        if next_state not in self.walls:\n",
    "            self.state = next_state\n",
    "        if self.is_terminal:\n",
    "            r = 1.\n",
    "        else:\n",
    "            r = 0.\n",
    "        return (self.state, r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StateActionValue:\n",
    "    def __init__(self):\n",
    "        self.Q = dict()\n",
    "\n",
    "    # the value(s) of state s and selecting action a (or all)\n",
    "    def __call__(self, s: State, a: Action = None):\n",
    "        if s not in self.Q:\n",
    "            return None\n",
    "        if a is None:\n",
    "            # value of all actions from s\n",
    "            return self.Q[s]\n",
    "        if a not in self.Q[s]:\n",
    "            return None\n",
    "        return self.Q[s][a]\n",
    "\n",
    "    def update(self, s, a, q):\n",
    "        if s not in self.Q:\n",
    "            self.Q[s] = dict()\n",
    "        self.Q[s][a] = q\n",
    "\n",
    "    def init(self, s, a_list, q=0.):\n",
    "        if s not in self.Q:\n",
    "            for a in a_list:\n",
    "                self.update(s, a, q)\n",
    "\n",
    "    def eps_greedy_policy(self, s: State, eps=0.1):\n",
    "        a_max = []\n",
    "        q_max = -999999.\n",
    "        for a in self.Q[s]:\n",
    "            if self.Q[s][a] - q_max > 1e-9:\n",
    "                q_max = self.Q[s][a]\n",
    "                a_max = [a]\n",
    "            elif abs(self.Q[s][a] - q_max) < 1e-9:\n",
    "                a_max.append(a)\n",
    "        random_prob = random.random()\n",
    "        if random_prob < eps:\n",
    "            return random.choice(list(self.Q[s].keys()))\n",
    "        return random.choice(a_max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GridWorldModel:\n",
    "    def __init__(self):\n",
    "        self.M = dict()\n",
    "\n",
    "    # the next state and reward\n",
    "    def __call__(self, s: State, a: Action):\n",
    "        if s not in self.M:\n",
    "            return None\n",
    "        if a not in self.M[s]:\n",
    "            return None\n",
    "        return self.M[s][a]\n",
    "\n",
    "    def update(self, s: State, a: Action, next_s: State, r):\n",
    "        if s not in self.M:\n",
    "            self.M[s] = dict()\n",
    "        self.M[s][a] = (next_s, r)\n",
    "\n",
    "    def get_random_transition(self):\n",
    "        s = random.choice(list(self.M.keys()))\n",
    "        a = random.choice(list(self.M[s].keys()))\n",
    "        next_s, r = self.M[s][a]\n",
    "        return (s, a, next_s, r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DynaAgent:\n",
    "    def __init__(self, gamma=0.95):\n",
    "        self.gamma = gamma\n",
    "        self.Q = StateActionValue()\n",
    "        self.M = GridWorldModel()\n",
    "\n",
    "    def Q_learning_update(self, env, s, a, next_s, r, alpha):\n",
    "        # one step Q-learning for the given transition\n",
    "        if next_s != env.goal:\n",
    "            next_a_max = self.Q.eps_greedy_policy(next_s, eps=0)\n",
    "            new_q = self.Q(s, a) + alpha * (r + self.gamma * self.Q(next_s, next_a_max) - self.Q(s, a))\n",
    "        else:\n",
    "            new_q = self.Q(s, a) + alpha * (r - self.Q(s, a))  # value of terminal state = 0 in this game\n",
    "        # update action values\n",
    "        self.Q.update(s, a, new_q)\n",
    "\n",
    "    \"\"\" Generate a single episode for a given environment\n",
    "        starting from whatever state the environment is in.\n",
    "        It goes until terminal state or max_steps was reached.\n",
    "    \"\"\"\n",
    "    def play_episode(\n",
    "            self,\n",
    "            env: GridWorld,\n",
    "            max_steps=5_000,  # avoid getting stuck forever (e.g. with greedy policies)\n",
    "            train=False,  # whether to update Q-values (and policy implicitly) during the episode\n",
    "            learn_model=True,  # whether to update the model of the environment during the episode\n",
    "            N=0,  # number of planning steps\n",
    "            epsilon=0.1,  # epsilon for epsilon-greedy policy updates\n",
    "            alpha=0.1,  # step size for each update of Q-values\n",
    "            keep_trace=False,  # store a list of tuples for all transitions\n",
    "        ):\n",
    "        trace = []\n",
    "        t = 0  # time steps\n",
    "        s = env.state\n",
    "        self.Q.init(s, env.legal_actions(s), q=0.)\n",
    "\n",
    "        while not env.is_terminal and t < max_steps:\n",
    "            a = self.Q.eps_greedy_policy(s, epsilon)\n",
    "            next_s, r = env.step(a)\n",
    "            if next_s != env.goal:\n",
    "                self.Q.init(next_s, env.legal_actions(next_s), q=0.)\n",
    "\n",
    "            if keep_trace:\n",
    "                trace.append((s, a, r, next_s))\n",
    "\n",
    "            # update model\n",
    "            if learn_model:\n",
    "                self.M.update(s, a, next_s, r)\n",
    "\n",
    "            # off-policy Q-learning from experience\n",
    "            if train:\n",
    "                self.Q_learning_update(env, s, a, next_s, r, alpha)\n",
    "\n",
    "                # N planning steps from model\n",
    "                for _ in range(N):\n",
    "                    random_s, random_a, random_next_s, random_r = self.M.get_random_transition()\n",
    "                    self.Q_learning_update(env, random_s, random_a, random_next_s, random_r, alpha)\n",
    "\n",
    "            s = next_s\n",
    "            t += 1\n",
    "        return t, trace\n",
    "\n",
    "    def train_controller(self, episodes=100):\n",
    "        epsilon = 0.1\n",
    "        alpha = 0.1\n",
    "        T_list = []\n",
    "        initial_wall = set([State(4, col) for col in range(1, 9)])\n",
    "        for e in range(episodes):\n",
    "            env = GridWorld(\n",
    "                rows=6,\n",
    "                cols=9,\n",
    "                inner_walls=initial_wall,\n",
    "                start=State(6, 4),\n",
    "                goal=State(1, 9),\n",
    "            )\n",
    "            T, _ = self.play_episode(env=env, train=True, learn_model=True, N=5, epsilon=epsilon, alpha=alpha, keep_trace=False)\n",
    "            T_list.append(T)\n",
    "        return T_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEWCAYAAACJ0YulAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/NK7nSAAAACXBIWXMAAAsTAAALEwEAmpwYAAApbElEQVR4nO3dd5xcZdn/8c+1szPZnU3ZJLuBVLIpIEUIGJAuUlQQBRERf6jIgw8W9MGfz6OiYkP9KVhQrPCI0lEsFCMKGIpITyCNQEgn2YRkN2Q32+v1++OcmcxutkzKzCR7vu/Xa16Zc58zM9fZzMw1dzn3be6OiIgIQFGhAxARkb2HkoKIiKQpKYiISJqSgoiIpCkpiIhImpKCiIikKSmIFIiZDTOzpWY2Ps+ve7iZPZXP15R9h5KC7HPMbI2ZtZhZg5nVmdlTZvZJMyvY+9nMDjGz+82sPozrETM7dpCHXQb8y903hs9xs5m5mR2T8bwzzGynLyYKn6vdzBozbjEAd18E1JnZe3b2eWXoU1KQfdV73H0EcADwfeBLwE2FCMTMpgNPAouBKmACcC/wcOYXfB8+CdzWq+wN4Dt7KLRr3X14xq0rY98dwCf20OvIEKKkIPs0d6939/uBDwIXm9lhZna0mW1K/TIGMLPzzGxheP+bZna3md0a/qp/ycxmZxx7pZmtDPctNbP3DRLGN4Gn3f2r7v6Guze4+/XA7cA1fT3AzKYA04Bne+26BTjczN62k3+KnfUYcJqZDcvx68g+RklBhgR3fw5YD5zk7s8DW4B3ZBzyEeDWjO33Ar8HyoH7gZ9n7FsJnASMAr4F3D5Iu/8ZwB/7KL8bOMnMSvrY92Zglbt39ipvBv4f8N2+XsjMfhk2mfV1W9Tr8E+b2RtmNt/M3p+5w92rgQ7goAHOSyJISUGGkg3AmPD+LcCHAcxsDPBO4M6MY//t7g+ETSq3AUekdrj7H919g7t3u/sfgOXAQM1AFcDGPso3ArGMmDKVAw39PN8NwBQzO7P3Dnf/tLuX93M7POPQ64GZwDjga8DNZnZCr6drCOMQSVNSkKFkIkGbPARNN+8xszLgAuCJVIdu6PWM+81AiZkVA5jZR81sQeoXOHAYwRc/YVNTquP2pPDxtUBfNYnxgBPUWnrbCozo6yTcvQ34dnjbJe7+grtvcfdOd3+AoA/hvF6HjQDqdvU1ZGhSUpAhwcyOJkgK/4Z088jTBF+EH2HHDt3+nucA4H+BzwBj3b0cWAJY+LyHZnTcPhE+7J/AB/p4uguAZ8Iv+d4WAVWpRNSH3xH8iu/xRW5mv+41oijz9tIAp+apcwifZyKQAJYN8BiJICUF2aeZ2UgzO5ugf+B2d1+csftW4IsE7fd/yfIpywi+QGvC57+EoKYwkG8Bx5vZd81sjJmNMLPPApcAX+/rAe6+HlhBP81SYV/DNwhGVWWWf7LXiKLM26Gp48zsfDMbbmZFZvYOgqa0+zOe6m3AI/0kLIkwJQXZV/3VzBqAdcBXgR8TfAlnuodgyOo97t6czZO6+1LgRwS1jE0ECeXJQR6zHDiRoF9iDUGTzLeB97n7Pwd46A0EtZj+3EXffRXZuAKoDmP5AfCf7v5Yxv6LgF/v4nPLEGZaZEeGMjNbCXxikC/nPf2ak4BngG+4e7/XToTDQV8ETuvV35Hr+A4HbnD34/L1mrLvUE1BhqxwGKYDj+TzdcOmoTOB8WY2fIDj2tz9kHwmhPB1FykhSH9UU5AhycweAw4BPuLuDxY4HJF9hpKCiIikqflIRETS+hsjvU+oqKjwqVOnFjoMEZF9yvz582vdvbKvfft0Upg6dSrz5s0rdBgiIvsUM1vb3z41H4mISJqSgoiIpCkpiIhImpKCiIikKSmIiEiakoKIiKQpKYiISFokk8Lza97ghw8uo6tbU3yIiGSKZFJ48bWt/PzRFTS3914zXUQk2iKZFEoTwYXcLR1dBY5ERGTvEs2kEI8B0NKupCAikimSSSGZCJOCagoiIj1EMimkagrNqimIiPQQzaQQ1hRalRRERHqIZlJQTUFEpE+RTArqUxAR6Vskk0KJRh+JiPQpkklBNQURkb5FMimkOprVpyAi0lMkk0JJsWoKIiJ9iWRSKCoySuJFtGjuIxGRHiKZFACSiWLVFEREeolsUiiNx9SnICLSS3STQiJGq2oKIiI9RDcpqKYgIrKD6CaFREwXr4mI9BLdpBBX85GISG+RTQrJhJqPRER6i2xSKI3HNCRVRKSX6CYF9SmIiOwguklBNQURkR1ENikkE0FScPdChyIistfIeVIws5iZvWhmc8LtKjN71sxWmNkfzCwRlg8Lt1eE+6fmMq6SRAx3aOvszuXLiIjsU/JRU7gCeDlj+xrgOnefAWwFLg3LLwW2huXXhcflTFJLcoqI7CCnScHMJgHvBn4TbhtwKvCn8JBbgHPD++eE24T7TwuPz4lSLbQjIrKDXNcUfgJ8EUi10YwF6tw9NWf1emBieH8isA4g3F8fHt+DmV1mZvPMbF5NTc0uB1aaKAbQ9NkiIhlylhTM7Gxgs7vP35PP6+43uvtsd59dWVm5y89Tml6nWX0KIiIpxTl87hOA95rZWUAJMBL4KVBuZsVhbWASUB0eXw1MBtabWTEwCtiSq+CS6SU5VVMQEUnJWU3B3b/s7pPcfSpwIfCIu18EPAqcHx52MXBfeP/+cJtw/yOew/GiJXH1KYiI9FaI6xS+BHzezFYQ9BncFJbfBIwNyz8PXJnLIFI1BV3VLCKyXS6bj9Lc/THgsfD+KuCYPo5pBT6Qj3ggo09BNQURkbRIX9EMuk5BRCRTZJNCSZgUtKaCiMh2kU0KpbqiWURkB5FNCvFYEfGYqU9BRCRDZJMChNNnq6YgIpIW7aSghXZERHqIdFJIJoppVvORiEhapJNCiZqPRER6iHRSCFZf09xHIiIpkU4K6mgWEekp2kkhEdN1CiIiGaKdFOIxXdEsIpIh0kkhqZqCiEgPkU4KJfGYrmgWEckQ6aSQ1MVrIiI9RDoplMZjdHY7HV1ap1lEBKKeFLSmgohID0oKaE0FEZGUSCcFrb4mItJTpJNCep1mJQURESDiSaEklRQ0/5GICBDxpJBMFAPQ0q7RRyIiEPGksH2dZtUUREQg6kkhkWo+Up+CiAhkkRTM7EAzm2tmS8Ltw83sqtyHlnvppKCOZhERILuawv8CXwY6ANx9EXBhLoPKl2RcNQURkUzZJIWkuz/Xq2xINMLrimYRkZ6ySQq1ZjYdcAAzOx/YmNOo8mRYcRFmuqJZRCSlOItjLgduBN5kZtXAauDDOY0qT8yM0rjWVBARSRk0Kbj7KuB0MysDity9Ifdh5U8yoTUVRERS+k0KZvb5fsoBcPcf5yimvCqJa00FEZGUgWoKI8J/DwKOBu4Pt98D9O543mdpoR0Rke36TQru/i0AM/sXcFSq2cjMvgn8LS/R5UFpPEazmo9ERIDsRh/tB7RnbLeHZUNCaSJGq2oKIiJAdqOPbgWeM7N7AAPOAW7OZVD5VBqPUdPYVugwRET2CoPWFNz9u8AlwFZgC3CJu39vsMeZWYmZPWdmC83sJTNLNUdVmdmzZrbCzP5gZomwfFi4vSLcP3W3zixLyUSx+hRERELZTojXBXRn3LLRBpzq7kcAs4B3mdmxwDXAde4+gyDRXBoefymwNSy/Ljwu5zT6SERku2wmxLsCuAOoAMYBt5vZZwd7nAcaw814eHPgVOBPYfktwLnh/XPCbcL9p1lq/GsO6ToFEZHtsulTuBR4q7s3AZjZNcDTwM8Ge6CZxYD5wAzgF8BKoM7dU3MnrQcmhvcnAusA3L3TzOqBsUBtr+e8DLgMYMqUKVmEP7DShK5oFhFJyab5yAiaj1K6wrJBuXuXu88CJgHHAG/a2QD7eM4b3X22u8+urKzc3aejNB6jrbOb7m7f7ecSEdnXZVNT+B3wbK/RRzftzIu4e52ZPQocB5SbWXFYW5gEVIeHVQOTgfVmVgyMIujYzqnMhXbKhmXz5xARGbqyGX30Y4LRR2+wffTRTwZ7nJlVmll5eL8UOAN4GXgUOD887GLgvvD+/eE24f5H3D3nP9+TWn1NRCRt0J/G4bTZL7n7C2b2duAkM1vt7nWDPHQ8cEvYr1AE3O3uc8xsKfB7M/sO8CLbax03AbeZ2QqCBJSXhXxK4lp9TUQkJZv2kj8Ds81sBvBrgl/0dwJnDfSgcIW2I/soX0XQv9C7vBX4QBbx7FGqKYiIbJdNR3N32P5/HvBzd/8CQS1gSCiNa/U1EZGUbJJCh5l9CPgoMCcsi+cupPxKdzQrKYiIZJUULiEYNfRdd19tZlXAbbkNK39SNYWWjiGx7LSIyG7JZuW1pcB/ZWyvJk9TUORDMhH8CVras529Q0Rk6Bpo5bW73f0CM1tMMD1FehfBLBaH5zy6PNjep6CagojIQDWFK8J/z85HIIWS6lNo1egjEZH++xTcfWP471qCGU+PAA4H2sKyISGVFDT6SEQku1lSP06wJvN5BFcaP2Nm/5HrwPJle0ezkoKISDYXr30BONLdtwCY2VjgKeC3uQwsX2JFRqK4SENSRUTIbkjqFqAhY7uBPExUl09aU0FEJJBNTWEFwSyp9xGMQjoHWGRmn4f0hHn7tNK41lQQEYHsksLK8JaSmtV0xJ4PpzBKVVMQEQGyu3jtWwBmlnT35tyHlH+lWqdZRATIbvTRceF016+E20eY2S9zHlkeJRNKCiIikF1H80+AdxJ2Lrv7QuDkHMaUdyXxGM1qPhIRySop4O7rehUNqW/QZCJGq2oKIiJZdTSvM7PjATezOMH0Fy/nNqz8Ko3HaNYsqSIiWdUUPglcDkwEqoFZ4faQUZoo1iypIiJkN/qoFrgoD7EUTGk8pgnxRETIsk9hqEsmYjS3d+Lugx8sIjKEKSkQXLzW7dDepSYkEYm2AZOCmRWZ2QX5CqZQ0jOlagSSiETcgEnB3buBL+YploJJramgqS5EJOqyaT76p5n9j5lNNrMxqVvOI8ujpBbaEREBsrtO4YPhv5nDUB2YtufDKYwSNR+JiADZDUmtykcghZRU85GICJDdhHhJM7vKzG4Mt2ea2dm5Dy1/1NEsIhLIpk/hd0A7cHy4XQ18J2cRFUCp+hRERIDsksJ0d78W6AAI11SwnEaVZ6magq5qFpGoyyYptJtZKUHnMmY2HWjLaVR5ppqCiEggm9FH3wD+AUw2szuAE4CP5TKofEvGgz+DOppFJOqyGX30sJm9ABxL0Gx0RThJ3pBRkggqTC3tmj5bRKItm5oCwNuAEwmakOLAPTmLqAASsSJiRaaagohEXjZDUn9JsKbCYmAJ8Akz+0WuA8snMwsW2lGfgohEXDY1hVOBgz2cV9rMbgFeymlUBVCa0JoKIiLZjD5aAUzJ2J4clg0onCvpUTNbamYvmdkVYfkYM3vYzJaH/44Oy83MrjezFWa2yMyO2pUT2lWqKYiIZJcURgAvm9ljZvYosBQYaWb3m9n9AzyuE/hvdz+EoJP6cjM7BLgSmOvuM4G54TbAmcDM8HYZ8KtdOqNdlEzEdEWziEReNs1HX9+VJ3b3jcDG8H6Dmb1MsM7zOcAp4WG3AI8BXwrLbw2bqZ4xs3IzGx8+T86VxGPqaBaRyMtmSOrju/siZjYVOBJ4Ftgv44v+dWC/8P5EYF3Gw9aHZT2SgpldRlCTYMqUzFat3aOagohIHpbjNLPhwJ+Bz7n7tsx9Ya1gpxZGdvcb3X22u8+urKzcY3GqT0FEJMdJwcziBAnhDnf/S1i8yczGh/vHA5vD8mqCTuyUSWFZXmj0kYjITiYFMxttZodneawBNwEvu/uPM3bdD1wc3r8YuC+j/KPhKKRjgfp89SeAagoiIpBFn4KZPQa8Nzx2PrDZzJ50988P8tATgI8Ai81sQVj2FeD7wN1mdimwFrgg3PcAcBbBcNdm4JKdOpPdlEyoo1lEJJvRR6PcfZuZfZxgdNA3zGzRYA9y93/T/xTbp/VxvNNzyc+8KlFHs4hIVs1HxWHb/wXAnBzHUzDJeDHtXd10dnUXOhQRkYLJJilcDTwIrHT3581sGrA8t2HlX2lqplQ1IYlIhGVzncIfgT9mbK8C3p/LoAqhNLF9TYURJfECRyMiUhjZzJI6zcz+amY1ZrbZzO4LawtDSmpJTvUriEiUZdN8dCdwNzAemEBQa7grl0EVQjJcklPNRyISZdkkhaS73+buneHtdqAk14HlW6qmoGsVRCTKshmS+nczuxL4PcGUFB8EHjCzMQDu/kYO48ub0rCm0KqkICIRlk1SSF1c9ole5RcSJIkh0b+gmoKISHajj6ryEUihqU9BRCS70UdJM7vKzG4Mt2ea2dm5Dy2/SjT6SEQkq47m3wHtwPHhdjXwnZxFVCCqKYiIZJcUprv7tUAHgLs30/+cRvusVEez+hREJMqySQrtZlZKuBiOmU0H2nIaVQGUFKumICKSzeijbwL/ACab2R0EU2LndVrrfCgqMkriRbS0dxY6FBGRgslm9NFDZjYfOJag2egKd6/NeWQFkEwUq6YgIpGWzeijue6+xd3/5u5z3L3WzObmI7h80+prIhJ1/dYUzKwESAIVZjaa7Z3LI4GJeYgt77ROs4hE3UDNR58APkcwCd58tieFbcDPcxtWYaimICJR129ScPefAj81s8+6+8/yGFPBlGpJThGJuH77FMzsaDPbP5UQzOyj4VoK16cmwxtqSuMxdTSLSKQN1NF8A8GVzJjZycD3gVuBeuDG3IeWf0nVFEQk4gbqU4hlTIv9QeBGd/8z8GczW5DzyApAfQoiEnUD1RRiZpZKGqcBj2Tsy+ait32ORh+JSNQN9OV+F/C4mdUCLcATAGY2g6AJachRTUFEom6g0UffDS9SGw885O4e7ioCPpuP4PItmQg6mt0dsyE355+IyKAGbAZy92f6KHs1d+EUVklqSc6O7vSsqSIiUZLNLKmRkYxrplQRiTYlhQzb11TQTKkiEk1KChlKE0FrmkYgiUhUKSlkKI1r9TURiTYlhQypdZqb2pQURCSalBQy7DeyBICN9S0FjkREpDCUFDJMGZOkyGB1bVOhQxERKQglhQyJ4iImj0myqkZJQUSiSUmhl2kVZaxSTUFEIkpJoZeqiuGsqW2iu9sHP1hEZIjJWVIws9+a2WYzW5JRNsbMHjaz5eG/o8NyCxfvWWFmi8zsqFzFNZiqyjJaOrrY1NBaqBBERAomlzWFm4F39Sq7Epjr7jOBueE2wJnAzPB2GfCrHMY1oGkVZQCsVr+CiERQzpKCu/8LeKNX8TnALeH9W4BzM8pv9cAzQLmZjc9VbAOpCpOC+hVEJIry3aewn7tvDO+/DuwX3p8IrMs4bn1YtgMzu8zM5pnZvJqamj0e4P4jSyiJF2lYqohEUsE6msP1GXa6N9fdb3T32e4+u7Kyco/HVVRkVFUMZ1VN4x5/bhGRvV2+k8KmVLNQ+O/msLwamJxx3KSwrCCmVZSppiAikZTvpHA/cHF4/2Lgvozyj4ajkI4F6jOamfKuqqKMdVtbaO/sLlQIIiIFkcshqXcBTwMHmdl6M7sU+D5whpktB04PtwEeAFYBK4D/BT6dq7iyUVVRRle3s25rcyHDEBHJuwGX49wd7v6hfnad1sexDlyeq1h2VlXl9mGp0yuHFzgaEZH80RXNfUhfq6B+BRGJGCWFPpQnE4wpS7CqViOQRCRalBT6UVVRptlSRSRylBT6UaVhqSISQUoK/aiqKGNzQxuNbZ2FDkVEJG+UFPqR6mxeo9qCiESIkkI/UsNSNTGeiESJkkI/po4tw0xTaItItCgp9KMkHmPCqFINSxWRSFFSGMC0So1AEpFoUVIYQFVFGatrmghm4RARGfqUFAZQVVFGQ1sntY3thQ5FRCQvlBQGUKU5kEQkYpQUBpCaIXW1OptFJCKUFAYwobyURKxIcyCJSGQoKQwgVmQcMDapC9hEJDKUFAahifFEJEqUFAZRVVnG2i1NdHXv3LDUlzbU88Digi0zLSKyS5QUBjGtooyOLqd6a0vWj2ls6+TSm+fxmTtf4LUtWudZRPYdSgqDmBaOQNqZ6S5++s9X2dTQSpEZNz6xMlehiYjscUoKg9jZaxVeeX0bv31yDRcePYXz3zKJP85bT01DWy5DFBHZY5QUBjG2LMGIkuKshqV2dztX3bOEUaVxvvjOg7js5Gm0d3Vz81Or8xCpiMjuU1IYhJkxLcsRSH9+YT3z1m7lyjPfxOiyBNMqh/OuQ/fn1qfX0tDakYdoZSh7amUtP3xwGd07OehBZGcoKWQhm2Gpdc3tfO/vrzD7gNGcf9SkdPkn3zadhtZO7nrutVyHKUPYqppGPnHrfH7+6Ap++6RqnpI7SgpZqKoYTnVdC60dXf0ec+2Dy6hv6eDb5x5GUZGly4+YXM7x08fymydW09bZ/+NF+tPc3smnbn+B4phx4owKrv3HMl55fVuhw5IhSkkhC9PCpTnXbOm7tvDia1u567nXuOT4qRw8fuQO+z91ynQ2N7Rx74vVOY1Thh5358t/Wcyrmxu4/kNH8tMLZzGyNM7nfr9gwB8pUbClsY0fP/wqG+qyHy4ug1NSyEJqBNLza7bu8EHs6nauuncJ40YM43NnHNjn40+cUcFhE0dyw+OrdvoiuH1NU1snyzc1FDqMIePWp9dy34IN/PcZB3LSzErGDh/Gtee/mVdeb+BHDy0rdHgFs2JzI+/75VNcP3c55/7iSRavry90SEOGkkIWqirKSMSK+Nq9Szj46//ghO8/wkW/eYav3LOYL/5pES9t2MbXzz6U4cOK+3y8mfHJt01nVW0TD730ep6jz59HX9nMGT9+nDOu+xefvmM+694YehfuvbalmadW1uYluc9f+wbfnrOU0w8ex6dPmZEuP/VN+/HhY6fwm3+v5qmVtTmPIx8a2zr59pylfOK2ebw6yI+Kp1bUct4vn6S5vZOffHAW8VgRF9zw9JD+bOWT7curis2ePdvnzZuXl9daWdPIkup6Vtc2saa2iTVbmlmzpYm65g5OOaiS333saMys38d3dTun/ugxykvj3Hv5CQMem2+ra5uY+/ImiswoT8bDW4Ly0uDf0cn4gPFuaWzj6jlLuW/BBmaMG85pB4/j1qfW0tXtXHpSFZe/fUa/CXNfUd/Swc/mLueWp9fQ0eVMHlPKxcdN5QOzJzOqNL5Lz9nV7RQZff5taxraOPtnT1ASj3H/Z07c4TVa2rt49/VP0NLRxT+uOJlRyR1j2Lytlb8veZ0DxiY5dtpYSuKxXYoz1x5dtpmr7lnChvoWkvEYrZ3dXPTWKXzu9AMZU5bocezd89bxlb8spqqijN9+7Ggmj0lS09DGx2+dx6L1dXz1rIO59MSqverzldLd7TjBRJuFZmbz3X12n/uUFHZPfXMHyWEx4rHBK113PLuWr96zhDs//laOn1GRh+j6V9PQxpxFG7h3wQYWrqsb8Njxo0p424GVnHJQJSfMqGBESfAF5O7cu6Caq/+6lMa2Tj59ygw+/fbpDCuO8Xp9K9c++Ap/eaGaiuHD+OI7D+L9b5mU1w9ER1c3y15vYMG6OlZsbqRyxDCmji3jgLFJplaUZZWoOru6uev5dVz38KtsbW7ngrdM5rjpY7nz2dd4bs0blMZjnHfURD52/FRm7jciq+d7YkUt971YzUNLN1Eaj3HE5HJmhbcjJpVTNizGh296lgXr6vjLp07gkAk79lMBLFxXx/t/9RRnvXk813/oSCD4P3lxXR03P7mGBxZvpDOs0ZTEizhu2lhOOWgcpxxUyQFjgybR7m5nU0Mra2qDHzlrtjQxLFbEyQdWMmtyOcUDvK+3NLbxr+U1vPhaHaXxGKOScUaHPyZS96sqyvpNRr1/TFzz/jdTVTGcn/zzVe549jXKEjGuOP1APnLsARQXGT98aBm/fGwlJ82s4BcXHcXIku2JsKW9i8/fvYC/L3mdi946hW+999ABY88Xd2dJ9TbuebGavy7awNamdiaPSTJ1bJIDxpZRVVHG1IoypleWMbG8NG/JTElhL9Ha0cWJ1zxKVUWSDx0zhbrmDuqa26lr6aCuuYOmtk72G1XC1LFJpo4N3ixTxiR7fKhaO7qoD4/f2txOXXMH9S3tbG3u2H6/qYOOrm5Ghb/0M3/9t3Z08bdFG/n3iqAJ5JDxIzn3yAmcffgEkolYEFNL8Nz1zR1saWpn3po3+PfyWhraOikuMt5ywGhOOWgcz6zawuOv1nDklHKuef/hHNjHl+LCdXVcPWcp89duZca44UwoL93hGAP2GzmMqRVlVI0t44CxZUytSJJMZPelva21k7rm4G+woa6FhevqWLCujiUb6mnt6AYgmYjR3N6zP6hi+DCqKrZ/OA/I+LsPH1bME8tr+Pacpby6qZG3Vo3ha2cfwmETR6Ufv6S6nlueWsN9CzfQ3tnNMVPHcMiEkcEHPjyXiaNLKS4yFq6v594Xq5mzaAO1je2MKo1z5mH709XtQdKqaST1UawcMYyahjZ+9IEjeP9bJjGQn81dzo8efpUfnH84xTHj5ifXsHB9PSOGFfOB2ZO58JjJbKhr4bFlNTy2bDNrwrm4Uk2ia99oSv+NABKxIjq7u+l2GFUa56SZFZxy0DjedmAlY8oSLFxfx2PLanh82WYWVdfjDmWJGB1dTntX9w7xFRcZB48fyRGTRzFr8mhmTS5nWkUZ9y3c/mPi8rfP4FOnBD8mUl7d1MC35yzlieW1TAu/OB95ZTMfOmYKV59zaJ8/wrq7nWsfXMavH1/JyQdWcsHsSeFnooOtTds/Z6WJWI/P2NSxScaUJQb9QnZ3Gto6qQ8/ew2tnSQTsXStemRpPP2j57Utzdy7oJp7F1SzqqaJRKyIUw6qZPq44azd0pROwpnvyYrhCY6YFP5AmFLO4ZPKd7kWOhglhb3IDY+v5Ht/fyW9bQYjS4Iv7WSimNfrW9ja3NFj//4jSwDY2tze4wPcWzxm6eaeeKwo/YFobOvscdzE8lLOmTWBc4+c2OcXeV86urp5Ye1WHnu1hseW1fDyxm0kEzG+8M6D+OhxUwesAbg7cxZt5LZn1tLeuWP83e5sqGultrHndCDjRgzr90PR1tmd/mD2Nqy4iMMmjgo+YFPKOXJyOZNGl9Lc3sXaLc2s3dLE6i1hM2D44dzcayqS0ck4W5s7mDymlK+edTDvPHT/fr803mhq567nXuOBxRtZU9tEU8YHvbjIGFUaZ0tTO4niIk4/eBznzJrIKQdV9vgSbGjtYPH6el5cV8fCdXUcOmEUV5w+s9+/aUpnVzcX3PA0L7xWB8D0yjI+dvxUzjtqEmV91ITW1Dbx+Ks1PLG8BiCoOaWTcZIJ5aU0tnbyxIrg//nxV2vS07SMGFZMQ1snRQazJpenax2HTRiFGbR2dFMX/iipa2lnS2M7SzduY+G6Ohatr0+/D0viRbR2dA/4YwKC982jyzbznTkvs3pLE18+803850nTBv3y/v1zr3HVvUvStSSA0niM8mScUaVxmtu7WL+1mcxuoRElxYwbMYyiPp67q9uDH2ItHQP2JaU+y8OHFVMdjoh6a9UYzj1yImcdNn6HJj53p6axjTW1zSzb1MCC1+pYuD6o1aZMGZNkWHHfNZ7/Om0m7zliwoB/i/5jVVLYa3R1Oy9v3EbZsGJGJ+OMKInv8IVa39yRrsqvqW1m7RtNFBcFX/jBr/845aXBl//I0jijy4L7pfFYnx+Yjq7udC2is9s5cNyIHtdS7IrN21pJFBdRnkwMfnCWGts6WVPbxNqwvyb4gt3xSx8gHisKmiqS8XTfR3kyTuWIYRy434ismvMyNbd3Bn/rMGGsrW1m5n7D+chxB/T48h5M6oO+dkszq2ubWLulidfr23hr1Rje9eb9ezR57Cnrtzbz68dX8s5D9+fEGRV7tAmiu9tZunEbj79aw/qtLRw3fSwnzahgdNnO/b93dTsraxpZsK6OxevredP4EVx49JSsmhM7urp5vb6VyWOSWb9edV0Lja2d6UTQuwmrvbOb9Vub05+xNVuadvhRklJklv7cjU5/BoPpb1rau6hraQ9r7h3UN7dT39LBQfuP5L2zJjCxj5rxYLa1drBoXT0L1m1l2aZGurr7/iF44dFTOPnAyp1+flBSEBGRDAMlhcL3xIiIyF5jr0oKZvYuM1tmZivM7MpCxyMiEjV7TVIwsxjwC+BM4BDgQ2Z2SGGjEhGJlr0mKQDHACvcfZW7twO/B84pcEwiIpGyNyWFicC6jO31YVkPZnaZmc0zs3k1NTV5C05EJAr2pqSQFXe/0d1nu/vsyspdG44lIiJ925uSQjUwOWN7UlgmIiJ5sjclheeBmWZWZWYJ4ELg/gLHJCISKXvVxWtmdhbwEyAG/NbdvzvI8TXA2l18uQpgaMw7vHOiet4Q3XPXeUdLNud9gLv32f6+VyWFfDKzef1d0TeURfW8IbrnrvOOlt09772p+UhERApMSUFERNKinBRuLHQABRLV84bonrvOO1p267wj26cgIiI7inJNQUREelFSEBGRtEgmhahM0W1mvzWzzWa2JKNsjJk9bGbLw39HFzLGXDCzyWb2qJktNbOXzOyKsHxIn7uZlZjZc2a2MDzvb4XlVWb2bPh+/0N4ceiQY2YxM3vRzOaE20P+vM1sjZktNrMFZjYvLNut93nkkkLEpui+GXhXr7IrgbnuPhOYG24PNZ3Af7v7IcCxwOXh//FQP/c24FR3PwKYBbzLzI4FrgGuc/cZwFbg0sKFmFNXAC9nbEflvN/u7rMyrk3Yrfd55JICEZqi293/BbzRq/gc4Jbw/i3AufmMKR/cfaO7vxDebyD4opjIED93D6RWfY+HNwdOBf4Ulg+58wYws0nAu4HfhNtGBM67H7v1Po9iUshqiu4hbD933xjefx3Yr5DB5JqZTQWOBJ4lAuceNqEsADYDDwMrgTp37wwPGarv958AXwRSq9yPJRrn7cBDZjbfzC4Ly3brfV68J6OTfYu7u5kN2THJZjYc+DPwOXffFvx4DAzVc3f3LmCWmZUD9wBvKmxEuWdmZwOb3X2+mZ1S4HDy7UR3rzazccDDZvZK5s5deZ9HsaYQ9Sm6N5nZeIDw380FjicnzCxOkBDucPe/hMWROHcAd68DHgWOA8rNLPUDcCi+308A3mtmawiag08FfsrQP2/cvTr8dzPBj4Bj2M33eRSTQtSn6L4fuDi8fzFwXwFjyYmwPfkm4GV3/3HGriF97mZWGdYQMLNS4AyC/pRHgfPDw4bcebv7l919krtPJfg8P+LuFzHEz9vMysxsROo+8A5gCbv5Po/kFc07O0X3vsrM7gJOIZhKdxPwDeBe4G5gCsG04xe4e+/O6H2amZ0IPAEsZnsb81cI+hWG7Lmb2eEEHYsxgh98d7v71WY2jeAX9BjgReDD7t5WuEhzJ2w++h93P3uon3d4fveEm8XAne7+XTMby268zyOZFEREpG9RbD4SEZF+KCmIiEiakoKIiKQpKYiISJqSgoiIpCkpiOwkM7vazE7fA8/TOPhRIvmlIakiBWJmje4+vNBxiGRSTUEEMLMPh2sRLDCzG8KJ5RrN7LpwbYK5ZlYZHnuzmZ0f3v9+uG7DIjP7YVg21cweCcvmmtmUsLzKzJ4O57//Tq/X/4KZPR8+JrUOQpmZ/S1cH2GJmX0wv38ViSIlBYk8MzsY+CBwgrvPArqAi4AyYJ67Hwo8TnBFeObjxgLvAw5198OB1Bf9z4BbwrI7gOvD8p8Cv3L3NwMbM57nHcBMgnlrZgFvMbOTCdbC2ODuR7j7YcA/9vCpi+xASUEETgPeAjwfTjt9GjCNYIqMP4TH3A6c2Otx9UArcJOZnQc0h+XHAXeG92/LeNwJwF0Z5SnvCG8vAi8QzGw6k2CajjPM7BozO8nd63fvNEUGp6mzRcAIftl/uUeh2dd6HdejA87dO83sGIIkcj7wGYIZOgfSVyeeAd9z9xt22GF2FHAW8B0zm+vuVw/y/CK7RTUFkWDJwvPDOelTa9weQPD5SM2y+X+Af2c+KFyvYZS7PwD8X+CIcNdTBLN1QtAM9UR4/8le5SkPAv8RPh9mNtHMxpnZBKDZ3W8HfgActSdOVmQgqilI5Ln7UjO7imAFqyKgA7gcaAKOCfdtJuh3yDQCuM/MSgh+7X8+LP8s8Dsz+wJQA1wSll8B3GlmXyJjOmN3fyjs13g6XAioEfgwMAP4gZl1hzF9as+euciONCRVpB8aMipRpOYjERFJU01BRETSVFMQEZE0JQUREUlTUhARkTQlBRERSVNSEBGRtP8P8OY9B/7/USQAAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "agent = DynaAgent(gamma=0.95)\n",
    "T_list = agent.train_controller(episodes=50)\n",
    "plt.plot(T_list)\n",
    "plt.xlabel('episodes')\n",
    "plt.ylabel('Steps per episode')\n",
    "plt.title('Dyna-Q (N=5)')\n",
    "# plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gi2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
